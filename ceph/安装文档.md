# 环境

* 系统环境：centos7.6 2003

* 节点：

  * ceph_node1: 
    * 安装组件：ceph-deploy,ceph-admin
    * 组件：mon1,mgr1,ods
    * ntp服务器
    * IP :192.168.10.15 
    * 硬盘：两块4T(后续会添加)
  * ceph_node2: 
    * 组件：mon2,mgr2,ods
    * IP :192.168.10.16
    * 硬盘：两块4T(后续会添加)
  * ceph_node3: 
  * 组件：mon2,mgr2,ods
    * IP :192.168.10.17 
    * 硬盘：两块6T 两块1T
  
  

# 部署

## 环境准备

1. 修改hostname

   node1执行：`# hostnamectl set-hostname node1 `

   node2执行：`# hostnamectl set-hostname node2 `

   node3执行：`# hostnamectl set-hostname node3`

2. 修改hosts

   三个节点机器分别修改host文件，修改完成后如下

   ```127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
   
   ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
   
   192.168.10.15 node1
   
   192.168.10.16 node2
   
   192.168.10.17 node3
   ```

3. 密码认证

   node1节点生成公钥：` # ssh-keygen` 一路默认确认即可

   ![image-20200711001028904](C:\Users\MAKJ\AppData\Roaming\Typora\typora-user-images\image-20200711001028904.png)

   拷贝公钥到三个节点，包括自己

   ```
   # ssh-copy-id -i /root/.ssh/id_rsa.pub node2
   # ssh-copy-id -i /root/.ssh/id_rsa.pub node3
   # ssh-copy-id -i /root/.ssh/id_rsa.pub node1
   ```

4. 关掉selinux

   三个节点机器分别修改selinux文件关闭selinux

   ```
   # vim /etc/selinux/
   # This file controls the state of SELinux on the system.
   # SELINUX= can take one of these three values:
   #     enforcing - SELinux security policy is enforced.
   #     permissive - SELinux prints warnings instead of enforcing.
   #     disabled - No SELinux policy is loaded.
   SELINUX=disabled
   # SELINUXTYPE= can take one of three values:
   #     targeted - Targeted processes are protected,
   #     minimum - Modification of targeted policy. Only selected processes are protected.
   #     mls - Multi Level Security protection.
   SELINUXTYPE=targeted
   
   # setenforce 0
   # getenforce
   Permissive
   ```

5. 关闭防火墙

   三个节点同时关闭防火墙并且删除

   ```
   # systemctl stop firewalld.service
   # systemctl disable firewalld.service
   Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.
   Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.
   # rpm -qa|grep firewall
   firewalld-filesystem-0.6.3-8.el7.noarch
   firewalld-0.6.3-8.el7.noarch
   python-firewall-0.6.3-8.el7.noarch
   # rpm -e --nodeps firewalld-0.6.3-8.el7.noarch firewalld-filesystem-0.6.3-8.el7.noarch
   
   ```

6. 配置ntp服务器

   三台节点同时安装ntp服务:` sudo yum install -y ntp`

   在node1启动ntp服务并且配置开机启动，注意修改时区为北京时间东八区()

   ```\
   # timedatectl set-timezone Asia/Shanghai
   # systemctl start ntpd
   # systemctl enable ntpd
   Created symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.
   ```

   在node2和node3节点上，修改ntp服务器的指向为node1

   ```
   # vim /etc/ntp.conf   #修改server
   ……
   # Use public servers from the pool.ntp.org project.
   # Please consider joining the pool (http://www.pool.ntp.org/join.html).
   #server 0.centos.pool.ntp.org iburst
   #server 1.centos.pool.
   ntp.org iburst
   #server 2.centos.pool.ntp.org iburst
   #server 3.centos.pool.ntp.org iburst
   server 192.168.10.15 iburst
   ……
   
   #修改完成后启动服务
   # systemctl start ntpd
   # systemctl enable ntpd
   #查看服务状态
   # ntpq -pn
        remote           refid      st t when poll reach   delay   offset  jitter
   ==============================================================================
   *192.168.10.15   193.182.111.143  3 u    6   64    1    0.237   -3.759   0.027
   ```

7. 三台节点全部配置阿里云yum源,包括Base、epel、ceph

   阿里云

   ```
   # curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                    Dload  Upload   Total   Spent    Left  Speed
   100  2523  100  2523    0     0  12040      0 --:--:-- --:--:-- --:--:-- 12014
   
   # curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
     % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                    Dload  Upload   Total   Spent    Left  Speed
   100   664  100   664    0     0   9454      0 --:--:-- --:--:-- --:--:--  9485
   
   # ll
   total 8
   -rw-r--r--. 1 root root 2523 Jul 11 16:49 CentOS-Base.repo
   -rw-r--r--. 1 root root  664 Jul 11 16:49 epel.repo
   ```

   ceph yum源 采用nautilus版本

   ```
   vim /etc/yum.repos.d/ceph.repo
   [norch]
   name=norch
   baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/
   enabled=1
   gpgcheck=0
   
   [x86_64]
   name=x86 64
   baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/
   enabled=1
   gpgcheck=0
   
   ```

   完毕后更新下系统：`sudo yum update`

   

## 开始部署

1. node1节点上安装ceph-deploy

   ```
   # yum install -y python-setuptools ceph-deploy
   # ceph-deploy -V
   usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME]
                      [--overwrite-conf] [--ceph-conf CEPH_CONF]
                      COMMAND ...
   ceph-deploy: error: too few arguments
   [root@node1 ~]# ceph-deploy -c
   usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME]
                      [--overwrite-conf] [--ceph-conf CEPH_CONF]
                      COMMAND ...
   ceph-deploy: error: too few arguments
   [root@node1 ~]# ceph-deploy -v
   usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME]
                      [--overwrite-conf] [--ceph-conf CEPH_CONF]
                      COMMAND ...
   ceph-deploy: error: too few arguments
   [root@node1 ~]# ceph-deploy
   usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME]
                      [--overwrite-conf] [--ceph-conf CEPH_CONF]
                      COMMAND ...
   
   Easy Ceph deployment
   
       -^-
      /   \
      |O o|  ceph-deploy v2.0.1
      ).-.(
     '/|||\`
     | '|` |
       '|`
   
   Full documentation can be found at: http://ceph.com/ceph-deploy/docs
   
   optional arguments:
     -h, --help            show this help message and exit
     -v, --verbose         be more verbose
     -q, --quiet           be less verbose
     --version             the current installed version of ceph-deploy
     --username USERNAME   the username to connect to the remote host
     --overwrite-conf      overwrite an existing conf file on remote host (if
                           present)
     --ceph-conf CEPH_CONF
                           use (or reuse) a given ceph.conf file
   
   commands:
     COMMAND               description
       new                 Start deploying a new cluster, and write a
                           CLUSTER.conf and keyring for it.
       install             Install Ceph packages on remote hosts.
       rgw                 Ceph RGW daemon management
       mgr                 Ceph MGR daemon management
       mds                 Ceph MDS daemon management
       mon                 Ceph MON Daemon management
       gatherkeys          Gather authentication keys for provisioning new nodes.
       disk                Manage disks on a remote host.
       osd                 Prepare a data disk on remote host.
       repo                Repo definition management
       admin               Push configuration and client.admin key to a remote
                           host.
       config              Copy ceph.conf to/from remote host(s)
       uninstall           Remove Ceph packages from remote hosts.
       purgedata           Purge (delete, destroy, discard, shred) any Ceph data
                           from /var/lib/ceph
       purge               Remove Ceph packages from remote hosts and purge all
                           data.
       forgetkeys          Remove authentication keys from the local directory.
       pkg                 Manage packages on remote hosts.
       calamari            Install and configure Calamari nodes. Assumes that a
                           repository with Calamari packages is already
                           configured. Refer to the docs for examples
                           (http://ceph.com/ceph-deploy/docs/conf.html)
   
   See 'ceph-deploy <command> --help' for help on a specific command
   
   ```
   


2. node1上初始化mon节点

   ```
   #创建一个目录
   # mkdir ceph-deploy
   # cd ceph-deploy/
   # 创建一个集群，指定访问网络和osd网络
   # ceph-deploy new --cluster-network=192.168.10.0/24 --public-network=192.168.10.0/24 node1
   [ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
   [ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy new --cluster-network=192.168.10.0/24 --public-network=192.168.10.0/24 node1
   [ceph_deploy.cli][INFO  ] ceph-deploy options:
   [ceph_deploy.cli][INFO  ]  username                      : None
   [ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fd9487022a8>
   [ceph_deploy.cli][INFO  ]  verbose                       : False
   [ceph_deploy.cli][INFO  ]  overwrite_conf                : False
   [ceph_deploy.cli][INFO  ]  quiet                         : False
   [ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd947e6ebd8>
   [ceph_deploy.cli][INFO  ]  cluster                       : ceph
   [ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
   [ceph_deploy.cli][INFO  ]  mon                           : ['node1']
   [ceph_deploy.cli][INFO  ]  public_network                : 192.168.10.0/24
   [ceph_deploy.cli][INFO  ]  ceph_conf                     : None
   [ceph_deploy.cli][INFO  ]  cluster_network               : 192.168.10.0/24
   [ceph_deploy.cli][INFO  ]  default_release               : False
   [ceph_deploy.cli][INFO  ]  fsid                          : None
   [ceph_deploy.new][DEBUG ] Creating new cluster named ceph
   [ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
   [node1][DEBUG ] connected to host: node1
   [node1][DEBUG ] detect platform information from remote host
   [node1][DEBUG ] detect machine type
   [node1][DEBUG ] find the location of an executable
   [node1][INFO  ] Running command: /usr/sbin/ip link show
   [node1][INFO  ] Running command: /usr/sbin/ip addr show
   [node1][DEBUG ] IP addresses found: [u'192.168.10.15']
   [ceph_deploy.new][DEBUG ] Resolving host node1
   [ceph_deploy.new][DEBUG ] Monitor node1 at 192.168.10.15
   [ceph_deploy.new][DEBUG ] Monitor initial members are ['node1']
   [ceph_deploy.new][DEBUG ] Monitor addrs are [u'192.168.10.15']
   [ceph_deploy.new][DEBUG ] Creating a random mon key...
   [ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
   [ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
   [root@node1 ceph-deploy]# ll
   total 12
   -rw-r--r-- 1 root root  263 Jul 12 02:03 ceph.conf
   -rw-r--r-- 1 root root 3007 Jul 12 02:03 ceph-deploy-ceph.log
   -rw------- 1 root root   73 Jul 12 02:03 ceph.mon.keyring
   
   ```

3. 手动安装ceph安装包(三个节点全部手动安装)，如果使用自动安装的话，ceph会修改yum源，导致安装非常慢

   ```
   # yum -y install ceph ceph-mon ceph-mgr ceph-radosgw ceph-mds
   ```

4. 初始化mon,会生成秘钥文件

   ```
   # ceph-deploy mon create-initial
   [ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
   [ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy mon create-initial
   [ceph_deploy.cli][INFO  ] ceph-deploy options:
   [ceph_deploy.cli][INFO  ]  username                      : None
   [ceph_deploy.cli][INFO  ]  verbose                       : False
   [ceph_deploy.cli][INFO  ]  overwrite_conf                : False
   [ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
   [ceph_deploy.cli][INFO  ]  quiet                         : False
   [ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcf869caf80>
   [ceph_deploy.cli][INFO  ]  cluster                       : ceph
   [ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fcf86e39848>
   [ceph_deploy.cli][INFO  ]  ceph_conf                     : None
   [ceph_deploy.cli][INFO  ]  default_release               : False
   [ceph_deploy.cli][INFO  ]  keyrings                      : None
   [ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts node1
   [ceph_deploy.mon][DEBUG ] detecting platform for host node1 ...
   [node1][DEBUG ] connected to host: node1
   [node1][DEBUG ] detect platform information from remote host
   [node1][DEBUG ] detect machine type
   [node1][DEBUG ] find the location of an executable
   [ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.8.2003 Core
   [node1][DEBUG ] determining if provided host has same hostname in remote
   [node1][DEBUG ] get remote short hostname
   [node1][DEBUG ] deploying mon to node1
   [node1][DEBUG ] get remote short hostname
   [node1][DEBUG ] remote hostname: node1
   [node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
   [node1][DEBUG ] create the mon path if it does not exist
   [node1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-node1/done
   [node1][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-node1/done
   [node1][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-node1.mon.keyring
   [node1][DEBUG ] create the monitor keyring file
   [node1][INFO  ] Running command: ceph-mon --cluster ceph --mkfs -i node1 --keyring /var/lib/ceph/tmp/ceph-node1.mon.keyring --setuser 167 --setgroup 167
   [node1][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-node1.mon.keyring
   [node1][DEBUG ] create a done file to avoid re-doing the mon deployment
   [node1][DEBUG ] create the init path if it does not exist
   [node1][INFO  ] Running command: systemctl enable ceph.target
   [node1][INFO  ] Running command: systemctl enable ceph-mon@node1
   [node1][WARNIN] Created symlink from /etc/systemd/system/ceph-mon.target.wants/ceph-mon@node1.service to /usr/lib/systemd/system/ceph-mon@.service.
   [node1][INFO  ] Running command: systemctl start ceph-mon@node1
   [node1][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node1.asok mon_status
   [node1][DEBUG ] ********************************************************************************
   [node1][DEBUG ] status for monitor: mon.node1
   [node1][DEBUG ] {
   [node1][DEBUG ]   "election_epoch": 3,
   [node1][DEBUG ]   "extra_probe_peers": [],
   [node1][DEBUG ]   "feature_map": {
   [node1][DEBUG ]     "mon": [
   [node1][DEBUG ]       {
   [node1][DEBUG ]         "features": "0x3ffddff8ffacffff",
   [node1][DEBUG ]         "num": 1,
   [node1][DEBUG ]         "release": "luminous"
   [node1][DEBUG ]       }
   [node1][DEBUG ]     ]
   [node1][DEBUG ]   },
   [node1][DEBUG ]   "features": {
   [node1][DEBUG ]     "quorum_con": "4611087854031667199",
   [node1][DEBUG ]     "quorum_mon": [
   [node1][DEBUG ]       "kraken",
   [node1][DEBUG ]       "luminous",
   [node1][DEBUG ]       "mimic",
   [node1][DEBUG ]       "osdmap-prune",
   [node1][DEBUG ]       "nautilus"
   [node1][DEBUG ]     ],
   [node1][DEBUG ]     "required_con": "2449958747315912708",
   [node1][DEBUG ]     "required_mon": [
   [node1][DEBUG ]       "kraken",
   [node1][DEBUG ]       "luminous",
   [node1][DEBUG ]       "mimic",
   [node1][DEBUG ]       "osdmap-prune",
   [node1][DEBUG ]       "nautilus"
   [node1][DEBUG ]     ]
   [node1][DEBUG ]   },
   [node1][DEBUG ]   "monmap": {
   [node1][DEBUG ]     "created": "2020-07-12 04:39:25.334373",
   [node1][DEBUG ]     "epoch": 1,
   [node1][DEBUG ]     "features": {
   [node1][DEBUG ]       "optional": [],
   [node1][DEBUG ]       "persistent": [
   [node1][DEBUG ]         "kraken",
   [node1][DEBUG ]         "luminous",
   [node1][DEBUG ]         "mimic",
   [node1][DEBUG ]         "osdmap-prune",
   [node1][DEBUG ]         "nautilus"
   [node1][DEBUG ]       ]
   [node1][DEBUG ]     },
   [node1][DEBUG ]     "fsid": "3ae84859-aea9-4b37-81c9-a36ee6b4ce1b",
   [node1][DEBUG ]     "min_mon_release": 14,
   [node1][DEBUG ]     "min_mon_release_name": "nautilus",
   [node1][DEBUG ]     "modified": "2020-07-12 04:39:25.334373",
   [node1][DEBUG ]     "mons": [
   [node1][DEBUG ]       {
   [node1][DEBUG ]         "addr": "192.168.10.15:6789/0",
   [node1][DEBUG ]         "name": "node1",
   [node1][DEBUG ]         "public_addr": "192.168.10.15:6789/0",
   [node1][DEBUG ]         "public_addrs": {
   [node1][DEBUG ]           "addrvec": [
   [node1][DEBUG ]             {
   [node1][DEBUG ]               "addr": "192.168.10.15:3300",
   [node1][DEBUG ]               "nonce": 0,
   [node1][DEBUG ]               "type": "v2"
   [node1][DEBUG ]             },
   [node1][DEBUG ]             {
   [node1][DEBUG ]               "addr": "192.168.10.15:6789",
   [node1][DEBUG ]               "nonce": 0,
   [node1][DEBUG ]               "type": "v1"
   [node1][DEBUG ]             }
   [node1][DEBUG ]           ]
   [node1][DEBUG ]         },
   [node1][DEBUG ]         "rank": 0
   [node1][DEBUG ]       }
   [node1][DEBUG ]     ]
   [node1][DEBUG ]   },
   [node1][DEBUG ]   "name": "node1",
   [node1][DEBUG ]   "outside_quorum": [],
   [node1][DEBUG ]   "quorum": [
   [node1][DEBUG ]     0
   [node1][DEBUG ]   ],
   [node1][DEBUG ]   "quorum_age": 2,
   [node1][DEBUG ]   "rank": 0,
   [node1][DEBUG ]   "state": "leader",
   [node1][DEBUG ]   "sync_provider": []
   [node1][DEBUG ] }
   [node1][DEBUG ] ********************************************************************************
   [node1][INFO  ] monitor: mon.node1 is running
   [node1][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node1.asok mon_status
   [ceph_deploy.mon][INFO  ] processing monitor mon.node1
   [node1][DEBUG ] connected to host: node1
   [node1][DEBUG ] detect platform information from remote host
   [node1][DEBUG ] detect machine type
   [node1][DEBUG ] find the location of an executable
   [node1][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node1.asok mon_status
   [ceph_deploy.mon][INFO  ] mon.node1 monitor has reached quorum!
   [ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
   [ceph_deploy.mon][INFO  ] Running gatherkeys...
   [ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpit1Ubk
   [node1][DEBUG ] connected to host: node1
   [node1][DEBUG ] detect platform information from remote host
   [node1][DEBUG ] detect machine type
   [node1][DEBUG ] get remote short hostname
   [node1][DEBUG ] fetch remote file
   [node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.node1.asok mon_status
   [node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-node1/keyring auth get client.admin
   [node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-node1/keyring auth get client.bootstrap-mds
   [node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-node1/keyring auth get client.bootstrap-mgr
   [node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-node1/keyring auth get client.bootstrap-osd
   [node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-node1/keyring auth get client.bootstrap-rgw
   [ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
   [ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
   [ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
   [ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
   [ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
   [ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
   [ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpit1Ubk
   
   #查看生成的秘钥文件
   # ll
   total 44
   -rw------- 1 root root   113 Jul 12 04:39 ceph.bootstrap-mds.keyring
   -rw------- 1 root root   113 Jul 12 04:39 ceph.bootstrap-mgr.keyring
   -rw------- 1 root root   113 Jul 12 04:39 ceph.bootstrap-osd.keyring
   -rw------- 1 root root   113 Jul 12 04:39 ceph.bootstrap-rgw.keyring
   -rw------- 1 root root   151 Jul 12 04:39 ceph.client.admin.keyring
   -rw-r--r-- 1 root root   263 Jul 12 02:03 ceph.conf
   -rw-r--r-- 1 root root 15354 Jul 12 04:39 ceph-deploy-ceph.log
   -rw------- 1 root root    73 Jul 12 02:03 ceph.mon.keyring
   
   ```
   
5. 将生成的秘钥文件和配置文件拷贝到其他两个节点

	```
   # ceph-deploy admin node1 node2 node3
   [ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
   [ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy admin node1 node2 node3
   [ceph_deploy.cli][INFO  ] ceph-deploy options:
   [ceph_deploy.cli][INFO  ]  username                      : None
   [ceph_deploy.cli][INFO  ]  verbose                       : False
   [ceph_deploy.cli][INFO  ]  overwrite_conf                : False
   [ceph_deploy.cli][INFO  ]  quiet                         : False
   [ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f85900d5560>
   [ceph_deploy.cli][INFO  ]  cluster                       : ceph
   [ceph_deploy.cli][INFO  ]  client                        : ['node1', 'node2', 'node3']
   [ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f8590975668>
   [ceph_deploy.cli][INFO  ]  ceph_conf                     : None
   [ceph_deploy.cli][INFO  ]  default_release               : False
   [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node1
   [node1][DEBUG ] connected to host: node1
   [node1][DEBUG ] detect platform information from remote host
   [node1][DEBUG ] detect machine type
   [node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
   [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node2
   [node2][DEBUG ] connected to host: node2
   [node2][DEBUG ] detect platform information from remote host
   [node2][DEBUG ] detect machine type
   [node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
   [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node3
   [node3][DEBUG ] connected to host: node3
   [node3][DEBUG ] detect platform information from remote host
   [node3][DEBUG ] detect machine type
   [node3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
	```

6. 执行ceph -s查看集群状态

   ```
   #  ceph -s
     cluster:
       id:     3ae84859-aea9-4b37-81c9-a36ee6b4ce1b
       health: HEALTH_OK
       
     services:
       mon: 1 daemons, quorum node1 (age 11m)
       mgr: no daemons active
       osd: 0 osds: 0 up, 0 in
       
     data:
       pools:   0 pools, 0 pgs
       objects: 0 objects, 0 B
       usage:   0 B used, 0 B / 0 B avail
       pgs:

   ```

7. 部署mgr监控节点，在node1上执行

   ```
   # ceph-deploy mgr create node1
   [ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
   [ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy mgr create node1
   [ceph_deploy.cli][INFO  ] ceph-deploy options:
   [ceph_deploy.cli][INFO  ]  username                      : None
   [ceph_deploy.cli][INFO  ]  verbose                       : False
   [ceph_deploy.cli][INFO  ]  mgr                           : [('node1', 'node1')]
   [ceph_deploy.cli][INFO  ]  overwrite_conf                : False
   [ceph_deploy.cli][INFO  ]  subcommand                    : create
   [ceph_deploy.cli][INFO  ]  quiet                         : False
   [ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f606d7758c0>
   [ceph_deploy.cli][INFO  ]  cluster                       : ceph
   [ceph_deploy.cli][INFO  ]  func                          : <function mgr at 0x7f606d738578>
   [ceph_deploy.cli][INFO  ]  ceph_conf                     : None
   [ceph_deploy.cli][INFO  ]  default_release               : False
   [ceph_deploy.mgr][DEBUG ] Deploying mgr, cluster ceph hosts node1:node1
   [node1][DEBUG ] connected to host: node1
   [node1][DEBUG ] detect platform information from remote host
   [node1][DEBUG ] detect machine type
   [ceph_deploy.mgr][INFO  ] Distro info: CentOS Linux 7.8.2003 Core
   [ceph_deploy.mgr][DEBUG ] remote host will use systemd
   [ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to node1
   [node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
   [node1][WARNIN] mgr keyring does not exist yet, creating one
   [node1][DEBUG ] create a keyring file
   [node1][DEBUG ] create path recursively if it doesn't exist
   [node1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.node1 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-node1/keyring
   [node1][INFO  ] Running command: systemctl enable ceph-mgr@node1
   [node1][WARNIN] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@node1.service to /usr/lib/systemd/system/ceph-mgr@.service.
   [node1][INFO  ] Running command: systemctl start ceph-mgr@node1
   [node1][INFO  ] Running command: systemctl enable ceph.target

   ```

   

8. 添加OSD磁盘,在node1上节点执行

   说明：Ceph到了L版本开始支持bluestore，之前均使用filestore，对于fielstore来说，将ssd以分区的形式指定为journal盘，用于加速是业界标准的方案，ssd的大小通常为10G；filestore逐渐废弃，推荐使用bluestore的模式，对于bluestore模式来说，其使用wal（日志分区）和db（元数据分区）的方式来存储，这两个分区分别存储BlueStore后端产生的元数据和日志文件，这样整个存储系统通过元数据对数据的操作效率极高，同时通过日志事务来维持系统的稳定性。官方推荐，wal通常为10G就足够了，db分区不低于数据盘的4%。
   以你环境来说，将ssd划分4个10G的分区用于wal日志存储，同时划分三个160G分区用于db，osd创建和添加的时候采用以下模式添加即可。
   ceph-deploy osd create ${node} --data /dev/sd${i} --block-wal /dev/nvme0n1p${j} --block-db /dev/nvme0n1p${k}

   

   三台节点先格式化ssd日志分区（分区链接：https://blog.csdn.net/qq_35036995/article/details/80532351）

   ```
   #### 三台节点根据磁盘格式化分区 wal每个分区20G db每个160G
   #格式化ssd 为gpt
   [root@node1 ~]# parted -s /dev/sdb mklabel gpt
   [root@node1 ~]# fdisk -l -u /dev/sdb
   WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.
   
   Disk /dev/sdb: 800.2 GB, 800166076416 bytes, 1562824368 sectors
   Units = sectors of 1 * 512 = 512 bytes
   Sector size (logical/physical): 512 bytes / 4096 bytes
   I/O size (minimum/optimal): 4096 bytes / 4096 bytes
   Disk label type: gpt
   Disk identifier: 213BC572-48D0-4DE9-93F4-5B41264FFA82
   
   ###最后分区结果如下，预留一下，防止最后扩充
   
   #         Start          End    Size  Type            Name
    1         2048     41945087     20G  Linux filesyste
    2     41945088    377489407    160G  Linux filesyste
    3    377489408    419432447     20G  Linux filesyste
    4    419432448    754976767    160G  Linux filesyste
    
   node1执行添加node1的osd
   [root@node1 ceph-deploy]# ceph-deploy osd create node1 --data /dev/sdc --block-wal /dev/sdb1 --block-db /dev/sdb2
   [root@node1 ceph-deploy]# ceph-deploy osd create node1 --data /dev/sdd --block-wal /dev/sdb3 --block-db /dev/sdb4
   
   node1执行添加node2的osd
   [root@node1 ceph-deploy]# ceph-deploy osd create node2 --data /dev/sdc --block-wal /dev/sdb1 --block-db /dev/sdb2
   [root@node1 ceph-deploy]# ceph-deploy osd create node2 --data /dev/sdd --block-wal /dev/sdb3 --block-db /dev/sdb4
   
   node1执行添加node3的osd
   [root@node1 ceph-deploy]# ceph-deploy osd create node3 --data /dev/sdc --block-wal /dev/sdb1 --block-db /dev/sdb2
   [root@node1 ceph-deploy]# ceph-deploy osd create node3 --data /dev/sdd --block-wal /dev/sdb3 --block-db /dev/sdb4
   ```

   

1. 进入 ceph-deploy目录。添加mon节点，将node2和node3添加到集群里

   ```
   # ceph-deploy mon add node2 --address 192.168.10.16
   # ceph-deploy mon add node3 --address 192.168.10.17
   # 查看节点添加成功并且已经进入选举集群内
   # ceph quorum_status --format json-pretty

   {
       "election_epoch": 16,
       "quorum": [
           0,
           1,
           2
       ],
       "quorum_names": [
           "node1",
           "node2",
           "node3"
       ],
       "quorum_leader_name": "node1",
       "quorum_age": 29,
       "monmap": {
           "epoch": 3,
           "fsid": "3ae84859-aea9-4b37-81c9-a36ee6b4ce1b",
           "modified": "2020-07-12 00:07:48.483888",
           "created": "2020-07-12 04:39:25.334373",
           "min_mon_release": 14,
           "min_mon_release_name": "nautilus",
           "features": {
               "persistent": [
                   "kraken",
                   "luminous",
                   "mimic",
                   "osdmap-prune",
                   "nautilus"
               ],
               "optional": []
           },
           "mons": [
               {
                   "rank": 0,
                   "name": "node1",
                   "public_addrs": {
                       "addrvec": [
                           {
                               "type": "v2",
                               "addr": "192.168.10.15:3300",
                               "nonce": 0
                           },
                           {
                               "type": "v1",
                               "addr": "192.168.10.15:6789",
                               "nonce": 0
                           }
                       ]
                   },
                   "addr": "192.168.10.15:6789/0",
                   "public_addr": "192.168.10.15:6789/0"
               },
               {
                   "rank": 1,
                   "name": "node2",
                   "public_addrs": {
                       "addrvec": [
                           {
                               "type": "v2",
                               "addr": "192.168.10.108:3300",
                               "nonce": 0
                           },
                           {
                               "type": "v1",
                               "addr": "192.168.10.108:6789",
                               "nonce": 0
                           }
                       ]
                   },
                   "addr": "192.168.10.108:6789/0",
                   "public_addr": "192.168.10.108:6789/0"
               },
               {
                   "rank": 2,
                   "name": "node3",
                   "public_addrs": {
                       "addrvec": [
                           {
                               "type": "v2",
                               "addr": "192.168.10.109:3300",
                               "nonce": 0
                           },
                           {
                               "type": "v1",
                               "addr": "192.168.10.109:6789",
                               "nonce": 0
                           }
                       ]
                   },
                   "addr": "192.168.10.109:6789/0",
                   "public_addr": "192.168.10.109:6789/0"
               }
           ]
       }
   }

   # ceph mon dump
   dumped monmap epoch 3
   epoch 3
   fsid 3ae84859-aea9-4b37-81c9-a36ee6b4ce1b
   last_changed 2020-07-12 00:07:48.483888
   created 2020-07-12 04:39:25.334373
   min_mon_release 14 (nautilus)
   0: [v2:192.168.10.15:3300/0,v1:192.168.10.15:6789/0] mon.node1
   1: [v2:192.168.10.108:3300/0,v1:192.168.10.108:6789/0] mon.node2
   2: [v2:192.168.10.109:3300/0,v1:192.168.10.109:6789/0] mon.node3

   ```

2. 扩容mgr 扩容 node2和node3

   ```
   # ceph-deploy mgr create node2 node3
   [ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
   [ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy mgr create node1 node2
   [ceph_deploy.cli][INFO  ] ceph-deploy options:
   [ceph_deploy.cli][INFO  ]  username                      : None
   [ceph_deploy.cli][INFO  ]  verbose                       : False
   [ceph_deploy.cli][INFO  ]  mgr                           : [('node1', 'node1'), ('node2', 'node2')]
   [ceph_deploy.cli][INFO  ]  overwrite_conf                : False
   [ceph_deploy.cli][INFO  ]  subcommand                    : create
   [ceph_deploy.cli][INFO  ]  quiet                         : False
   [ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4aa035f908>
   [ceph_deploy.cli][INFO  ]  cluster                       : ceph
   [ceph_deploy.cli][INFO  ]  func                          : <function mgr at 0x7f4aa0322578>
   [ceph_deploy.cli][INFO  ]  ceph_conf                     : None
   [ceph_deploy.cli][INFO  ]  default_release               : False
   [ceph_deploy.mgr][DEBUG ] Deploying mgr, cluster ceph hosts node1:node1 node2:node2
   [node1][DEBUG ] connected to host: node1
   [node1][DEBUG ] detect platform information from remote host
   [node1][DEBUG ] detect machine type
   [ceph_deploy.mgr][INFO  ] Distro info: CentOS Linux 7.8.2003 Core
   [ceph_deploy.mgr][DEBUG ] remote host will use systemd
   [ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to node1
   [node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
   [node1][DEBUG ] create path recursively if it doesn't exist
   [node1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.node1 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-node1/keyring
   [node1][INFO  ] Running command: systemctl enable ceph-mgr@node1
   [node1][INFO  ] Running command: systemctl start ceph-mgr@node1
   [node1][INFO  ] Running command: systemctl enable ceph.target
   [node2][DEBUG ] connected to host: node2
   [node2][DEBUG ] detect platform information from remote host
   [node2][DEBUG ] detect machine type
   [ceph_deploy.mgr][INFO  ] Distro info: CentOS Linux 7.8.2003 Core
   [ceph_deploy.mgr][DEBUG ] remote host will use systemd
   [ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to node2
   [node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
   [node2][WARNIN] mgr keyring does not exist yet, creating one
   [node2][DEBUG ] create a keyring file
   [node2][DEBUG ] create path recursively if it doesn't exist
   [node2][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.node2 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-node2/keyring
   [node2][INFO  ] Running command: systemctl enable ceph-mgr@node2
   [node2][WARNIN] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@node2.service to /usr/lib/systemd/system/ceph-mgr@.service.
   [node2][INFO  ] Running command: systemctl start ceph-mgr@node2
   [node2][INFO  ] Running command: systemctl enable ceph.target
   
   ```

   


# 运维 

## 删除osd

​	参考：https://www.cnblogs.com/ajunyu/p/11165950.html

1. ceph-deploy节点把 OSD 踢出集群 

   ```
   ceph osd out osd.4
   ```

2. 在相应的节点，停止ceph-osd服务

   ```
   systemctl stop ceph-osd@4.service
   systemctl disable ceph-osd@4.service
   ```

3. 删除 CRUSH 图的对应 OSD 条目，它就不再接收数据了

   ```
   ceph osd crush remove osd.4
   ```

4. 删除 OSD 认证密钥

   ```
   ceph auth del osd.4
   ```

5. 通过ceph命令强行标记为down

   ```
    ceph osd down osd.4
   ```

6. 删除osd.4

   ```
   ceph osd rm osd.4
   ```

7. 最后通过命令查看最新osd

   ```
   ceph osd tree
   ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
   -1       12.73537 root default
   -3        3.63869     host node1
    0   hdd  3.63869         osd.0      up  1.00000 1.00000
   -5        3.63869     host node2
    1   hdd  3.63869         osd.1      up  1.00000 1.00000
   -7        5.45799     host node3
    2   hdd  5.45799         osd.2      up  1.00000 1.00000
   
   ```

   



# 问题

## 问题1

创建osd的时候：Failed to execute command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/sdc

解决：

​	ceph-deploy重置sdb分区： ceph-deploy disk zap node2 /dev/sdb

​	node2节点删掉分区： pvremove /dev/sdb

​	然后在创建： ceph-deploy osd create node2 --data /dev/sdb

​	如果卸载过程中还报错直接手动执行命令：dd if=/dev/zero of=/dev/sdc bs=512K count=1 然后重启



## 问题2

ceph -s 出现 clock skew detected on mon.node2 问题

```
[root@node1 ceph-deploy]# ceph -s
  cluster:
    id:     3ae84859-aea9-4b37-81c9-a36ee6b4ce1b
    health: HEALTH_WARN
    		clock skew detected on mon.node2
            1/3 mons down, quorum node1,node2

  services:
    mon: 3 daemons, quorum node1,node2 (age 0.431165s), out of quorum: node3
    mgr: node1(active, since 30m), standbys: node2, node3
    osd: 6 osds: 6 up (since 5m), 6 in (since 5m)

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   1.1 TiB used, 25 TiB / 27 TiB avail
    pgs:

```

解决：

**1. 在admin部署节点修改配置参数：**

```
# vi ~/my-cluster/ceph.conf
```

在global字段下添加：

```
mon clock drift allowed = 2
mon clock drift warn backoff = 30    
```

**2. 向需要同步的mon节点推送配置文件：**

```
# ceph-deploy --overwrite-conf config push node{1..3}
```

这里是向node1 node2 node3推送，也可以后跟其它不连续节点
**3. 重启mon服务（centos7环境下）**

```
# systemctl restart ceph-mon.target
```

**4.验证：**

```
# ceph -s
```

问题解决



ceph清除

```
# ceph-deploy purge node1 node2 node3
# ceph-deploy purgedata node1 node2 node3
# ceph-deploy forgetkeys
# rm -f ceph*
```





