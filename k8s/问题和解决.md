# 问题1

` Warning  FailedScheduling  2m (x53 over 57m)  default-scheduler  0/3 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.`

解决：

 3个节点有pod不能容忍的污点{[node](https://so.csdn.net/so/search?q=node&spm=1001.2101.3001.7020)-role.kubernetes.io/master:}。

此问题是master节点不参与调度，改为可以调度就可以

`kubectl taint nodes --all node-role.kubernetes.io/master-`



# **节点宕机后驱离pod到健康的节点**

默认情况下：

一个节点宕机后，

Kubernetes 会自动给 Pod 添加一个 key 为 node.kubernetes.io/not-ready 的容忍度 并配置 tolerationSeconds=300，除非用户提供的 Pod 配置中已经已存在了 key 为 node.kubernetes.io/not-ready 的容忍度。

同样，Kubernetes 会给 Pod 添加一个 key 为 node.kubernetes.io/unreachable 的容忍度 并配置 tolerationSeconds=300，除非用户提供的 Pod 配置中已经已存在了 key 为 node.kubernetes.io/unreachable 的容忍度。

这种自动添加的容忍度意味着在其中一种问题被检测到时 Pod 默认能够继续停留在故障节点运行 5 分钟。

通过改变容忍度的时间定义，可以更快速的自动迁移pod到健康的节点：

      tolerations:
      - key: node.kubernetes.io/not-ready
        operator: Exists
        effect: NoExecute
        tolerationSeconds: 2
      - key: node.kubernetes.io/unreachable
        operator: Exists
        effect: NoExecute
        tolerationSeconds: 2

deployment的yaml代码如下：

```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: www
  name: www
  namespace: webapp
spec:
  progressDeadlineSeconds: 600
  replicas: 6
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: www
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: www
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      tolerations:
      - key: node.kubernetes.io/not-ready
        operator: Exists
        effect: NoExecute
        tolerationSeconds: 2
      - key: node.kubernetes.io/unreachable
        operator: Exists
        effect: NoExecute
        tolerationSeconds: 2
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
```



备注：

当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：

node.kubernetes.io/not-ready：节点未准备好。这相当于节点状态 Ready 的值为 "False"。
node.kubernetes.io/unreachable：节点控制器访问不到节点. 这相当于节点状态 Ready 的值为 "Unknown"。
node.kubernetes.io/memory-pressure：节点存在内存压力。
node.kubernetes.io/disk-pressure：节点存在磁盘压力。
node.kubernetes.io/pid-pressure: 节点的 PID 压力。
node.kubernetes.io/network-unavailable：节点网络不可用。
node.kubernetes.io/unschedulable: 节点不可调度。



node.cloudprovider.kubernetes.io/uninitialized：如果 kubelet 启动时指定了一个 "外部" 云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。

©著作权归作者所有：来自51CTO博客作者雲皓的原创作品，请联系作者获取转载授权，否则将追究法律责任
节点宕机后驱离pod到健康的节点
https://blog.51cto.com/u_15307556/3177669



# Kubernetes Kubelet 状态更新机制 node-status-update-frequency

当 Kubernetes 中 Node 节点出现状态异常的情况下，节点上的 Pod 会被重新调度到其他节点上去，但是有的时候我们会发现节点 Down 掉以后，Pod 并不会立即触发重新调度，这实际上就是和 Kubelet 的状态更新机制密切相关的，Kubernetes 提供了一些参数配置来触发重新调度到嗯时间，下面我们来分析下 Kubelet 状态更新的基本流程。

kubelet 自身会定期更新状态到 apiserver，通过参数--node-status-update-frequency指定上报频率，默认是 10s 上报一次。
kube-controller-manager 会每隔--node-monitor-period时间去检查 kubelet 的状态，默认是 5s。
当 node 失联一段时间后，kubernetes 判定 node 为 notready 状态，这段时长通过--node-monitor-grace-period参数配置，默认 40s。
当 node 失联一段时间后，kubernetes 判定 node 为 unhealthy 状态，这段时长通过--node-startup-grace-period参数配置，默认 1m0s。
当 node 失联一段时间后，kubernetes 开始删除原 node 上的 pod，这段时长是通过--pod-eviction-timeout参数配置，默认 5m0s。
kube-controller-manager 和 kubelet 是异步工作的，这意味着延迟可能包括任何的网络延迟、apiserver 的延迟、etcd 延迟，一个节点上的负载引起的延迟等等。因此，如果--node-status-update-frequency设置为5s，那么实际上 etcd 中的数据变化会需要 6-7s，甚至更长时间。

Kubelet在更新状态失败时，会进行nodeStatusUpdateRetry次重试，默认为 5 次。

Kubelet 会在函数tryUpdateNodeStatus中尝试进行状态更新。Kubelet 使用了 Golang 中的http.Client()方法，但是没有指定超时时间，因此，如果 API Server 过载时，当建立 TCP 连接时可能会出现一些故障。

因此，在nodeStatusUpdateRetry * --node-status-update-frequency时间后才会更新一次节点状态。

同时，Kubernetes 的 controller manager 将尝试每--node-monitor-period时间周期内检查nodeStatusUpdateRetry次。在--node-monitor-grace-period之后，会认为节点 unhealthy，然后会在--pod-eviction-timeout后删除 Pod。

kube proxy 有一个 watcher API，一旦 Pod 被驱逐了，kube proxy 将会通知更新节点的 iptables 规则，将 Pod 从 Service 的 Endpoints 中移除，这样就不会访问到来自故障节点的 Pod 了。

配置
对于这些参数的配置，需要根据不同的集群规模场景来进行配置。

社区默认的配置

参数	值
–node-status-update-frequency	10s
–node-monitor-period	5s
–node-monitor-grace-period	40s
–pod-eviction-timeout	5m
快速更新和快速响应

参数	值
–node-status-update-frequency	4s
–node-monitor-period	2s
–node-monitor-grace-period	20s
–pod-eviction-timeout	30s
在这种情况下，Pod 将在 50s 被驱逐，因为该节点在 20s 后被视为Down掉了，--pod-eviction-timeout在 30s 之后发生，但是，这种情况会给 etcd 产生很大的开销，因为每个节点都会尝试每 2s 更新一次状态。

如果环境有1000个节点，那么每分钟将有15000次节点更新操作，这可能需要大型 etcd 容器甚至是 etcd 的专用节点。

如果我们计算尝试次数，则除法将给出5，但实际上每次尝试的 nodeStatusUpdateRetry 尝试将从3到5。 由于所有组件的延迟，尝试总次数将在15到25之间变化。

中等更新和平均响应

参数	值
–node-status-update-frequency	20s
–node-monitor-period	5s
–node-monitor-grace-period	2m
–pod-eviction-timeout	1m
这种场景下会 20s 更新一次 node 状态，controller manager 认为 node 状态不正常之前，会有 2m*60⁄20*5=30 次的 node 状态更新，Node 状态为 down 之后 1m，就会触发驱逐操作。

如果有 1000 个节点，1分钟之内就会有 60s/20s*1000=3000 次的节点状态更新操作。

低更新和慢响应
参数	值
–node-status-update-frequency	1m
–node-monitor-period	5s
–node-monitor-grace-period	5m
–pod-eviction-timeout	1m
Kubelet 将会 1m 更新一次节点的状态，在认为不健康之后会有 5m/1m*5=25 次重试更新的机会。Node为不健康的时候，1m 之后 pod开始被驱逐。

可以有不同的组合，例如快速更新和慢反应以满足特定情况。

原文链接: 
https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md
https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md

  configuration - Changing the default behavior of Kubernetes - Stack Overflow

I have setup a K8S cluster (1 master and 2 slaves) using Kubeadm on my laptop.

Deployed 6 replicas of a pod. 3 of them got deployed to each of the slaves.
Did a shutdown of one of the slave.
It took ~6 minutes for the 3 pods to be scheduled on the running node.
Initially, I thought that it had to do something with the K8S setup. After some digging found out, it's because of the defaults in the K8S for Controller Manager and Kubelet as mentioned here. It made sense. I checked out the K8S documentation on where to change the configuration properties and also checked the configuration files on the cluster node, but couldn't figure it out.

kubelet: node-status-update-frequency=4s (from 10s)
controller-manager: node-monitor-period=2s (from 5s)
controller-manager: node-monitor-grace-period=16s (from 40s)
controller-manager: pod-eviction-timeout=30s (from 5m)
Could someone point out what needs to be done to make the above-mentioned configuration changes permanent and also the different options for the same?

Answer
On the kubelet change this file on all your nodes:

/var/lib/kubelet/kubeadm-flags.env
Add the option at the end or anywhere on this line:

KUBELET_KUBEADM_ARGS=--cgroup-driver=cgroupfs --cni-bin-dir=/opt/cni/bin 
--cni-conf-dir=/etc/cni/net.d --network-plugin=cni 
--resolv-conf=/run/systemd/resolve/resolv.conf 
--node-status-update-frequency=10s <== add this
On your kube-controller-manager change on the master the following file:

/etc/kubernetes/manifests/kube-controller-manager.yaml
In this section:

  containers:
  - command:
    - kube-controller-manager
    - --address=127.0.0.1
    - --allocate-node-cidrs=true
    - --cloud-provider=aws
    - --cluster-cidr=192.168.0.0/16
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --node-cidr-mask-size=24
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --use-service-account-credentials=true
    - –-node-monitor-period=5s  <== add this line
    On your master do a sudo systemctl restart docker On all your nodes do a sudo systemctl restart kubelet

You should have the new configs take effect.

Hope it helps.
————————————————
版权声明：本文为CSDN博主「富士康质检员张全蛋」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_34556414/article/details/122651801